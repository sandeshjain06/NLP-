{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056067c5",
   "metadata": {},
   "source": [
    "## What is NLP\n",
    "\n",
    "\n",
    "- Natural Language Processing (NLP) is a part of AI (artificial intelligence) that deals with understanding and processing of human language. In real time, majority of data exists in the unstructured form such us text, videos, images. Mass of data in unstructured category, will be in textual form. To process this textual data's with machine learning algorithms, NLP comes in to play.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- NLP is subfield of langauges , computer science and AI concerned with interactions between computers and  human language , in particular how to program computers to process and analyze large amounts of natural language data .\n",
    "\n",
    "\n",
    "## Real World  Applications\n",
    "\n",
    "    1. Contextual Advertisements - Providing diff ads to diff customer based on their behaviour. \n",
    "    2. Email Spam Filtering , Smart Reply\n",
    "    3. Social Media - Removing Adult Content, Opinion Mining\n",
    "    - Eg Twitter -  based on customer opinion we can predict which party is going to win .\n",
    "    4. Search Engines \n",
    "    5. Chatbots \n",
    " \n",
    "## Libraries used in NLP \n",
    "\n",
    "\n",
    "    1. Natural Language Toolkit (NLTK)\n",
    "    2. SpaCY\n",
    "    3. Gensim\n",
    "    4. Standford CoreNLP\n",
    "    5. TextBlob\n",
    " \n",
    " \n",
    "## Common NLP Tasks\n",
    "\n",
    "    1. Text / Document Classification -  If u have a dataset of news headlines and want to classify that into category such as sports,entertainment,politics,education.\n",
    "\n",
    "    2. Sentiment Analysis \n",
    "    \n",
    "    3. Information Retrieval - From a text if u have to extract the entities , search engines uses such algorithm \n",
    "\n",
    "    4. Parts of Speech Tagging - for every word , we assign a parts of speech so that it can understand the sentence, eg of this would be chatbot.\n",
    "\n",
    "    5. Language Detection and Machine Translation - Converted language to user specified language.\n",
    "\n",
    "    6. Conversational Agents : eg - Apple SIRI , Amazon Alexa , Google Assistant , Microsoft cortona .\n",
    "\n",
    "    7. Text Summarization : eg:Inshorts\n",
    "\n",
    "    8. Topic Modelling - From large text , we can extract the abstract topics , means from the paragraph the model can say \n",
    "    about the context .\n",
    "\n",
    "    9. Text Generation - Keywords automatically predicted the next words.\n",
    "\n",
    "    10. Spell checking and grammer correction\n",
    "\n",
    "    11. Text Parsing \n",
    "\n",
    "    12. Speech to Text \n",
    "\n",
    "\n",
    "## Approaches to NLP\n",
    "\n",
    "    Heuristic Approaches\n",
    "    - Regular Expressions\n",
    "    - WordNet\n",
    "    - Open Mind Common Sense\n",
    "    \n",
    "    Machine Learning Approach\n",
    "    - Used in Text Vectorization - text to numbers.\n",
    "    - Naive Bayes , Logistic Regression , SVM , LDA - Linear Discriminant Analysis , Hidden markov Models\n",
    "    \n",
    "    Deep Learning Approach\n",
    "    - In machine learning,when text is converted into numbers , it does not give importance to the sequence of data , \n",
    "    so for that deep learning comes into the picture. \n",
    "    - RNN\n",
    "    - LSTM\n",
    "    - GRU / CNN\n",
    "    - Transformers(BERT)\n",
    "    - AutoEncoders \n",
    "    \n",
    "    Challenges in NLP\n",
    "    - Ambiguity\n",
    "    - Contextual Words \n",
    "    - Colloquialisms and Slang\n",
    "    - Synonyms\n",
    "    - Irony , Sarcasm and toanl diff\n",
    "    - Spelling Errors\n",
    "    - Creativity\n",
    "    - Diversity\n",
    "    \n",
    "    \n",
    "## What is NLP Pipeline \n",
    "\n",
    "     - Sets of steps followed to build an end to end NLP software.\n",
    "     - NLP software consists of following steps : \n",
    "       Data Acquisition\n",
    "       Text Preparation\n",
    "       Feature Engineering\n",
    "       Data Modelling\n",
    "       Model Deployment\n",
    "    \n",
    "\n",
    "## Data Acquistion\n",
    "\n",
    "    1.Data is available\n",
    "    - Complete data is available in form of csv. \n",
    "    - Data is available on databases.\n",
    "    - Data is less \n",
    "        So go with Data augmentation, below are the techniques used\n",
    "        - Bigram \n",
    "        - Back translate\n",
    "        - Add noise to data.\n",
    "        - change of synomyms.\n",
    "    \n",
    "    2.Data is available on some other sites\n",
    "    - Public Dataset\n",
    "    - Web API  - RapidAPI where list of websites API are given.\n",
    "    - Web Scraping\n",
    "    - Data is available in form of PDF , Image , Audio.\n",
    "    \n",
    "    3.Data is not available , completely new projects.\n",
    "    - We have to manually take the review and check the sentiment,once the data is increased then we can move to above process.\n",
    "    \n",
    "    \n",
    "## Text Preprocessing\n",
    "\n",
    "    1.Basic cleaning\n",
    "    \n",
    "    \n",
    "    - Html tag cleaning\n",
    "    - Emoji \n",
    "    - Spelling check.\n",
    "    \n",
    "    2.Basic text preprocessing\n",
    "    \n",
    "    - Basic \n",
    "      Tokenization - sentence and word tokenization\n",
    "    - Optional\n",
    "     Stop words removal , Stemming , lower case , language detection , removing digits and punctuation.\n",
    "      \n",
    "    3.Advanced preprocessing\n",
    "    \n",
    "    1. Parts of Speech Tagging - eg.chatbox - understand the parts of speech of the sentence.\n",
    "    \n",
    "    2. Parsing - Understand the sentence , syntectic structure\n",
    "    \n",
    "    3. Core Reference Resolution\n",
    "    eg , Hi,my name is Sandesh and i am not a terrorist.\n",
    "    - Here 'i' also denotes about me but machine will not able to understand it. \n",
    "    \n",
    "    Feature Engineering\n",
    "    - Convert text into numbers by diff technique.\n",
    "    - ML Pipeline - model accuracy can be interpretable\n",
    "    - Deep learning pipeline - Not interpretable as deep learning automatically creates the features which is not known to users.\n",
    "    \n",
    "    Data Modelling\n",
    "    - Heuristic / ML Algorithm / DL Algorithm / Cloud API.\n",
    "    - Depends on amount of data , nature of problem.\n",
    "    \n",
    "    Eg : Email Spam Classifier.\n",
    "    - Heuristic approach means after analyzing the data , we can apply some technique such as check from which email id we are getting the mail , \n",
    "    some words which are consistently used in spam message.\n",
    "    - ML algorithm , here we have to apply ML algorithm to the data.\n",
    "    - In DL, nowadays transfer learning technique is used to train the \n",
    "    model , the algorithm is already available on internet we have to apply that on our data .\n",
    "    Here we want more data.\n",
    "    - In cloud API , there are existing algo available online which we can directly apply. \n",
    "    \n",
    "    Model Deployment\n",
    "    - Intrinsic  - evaluation about the model .\n",
    "    - Extrinsic - means after deployment how many customer are using the model\n",
    "    - To check that we have perplexity term which can help us to determine that .\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f36f93",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "    Corpus -     Collection of all words as well as duplicate words in the data. \n",
    "    Vocabulary - Unique words in the entire data. \n",
    "    Document - \n",
    "    Eg - sentiment analysis of twitter data , every review is a document. \n",
    "    \n",
    "    To Convert text into numbers.\n",
    "    1.One-Hot Encoding\n",
    "    2.Bag of words\n",
    "    3.Ngram\n",
    "    4.TF-IDF\n",
    "    5.Custom Features\n",
    "    6.Word2Vec - embedding\n",
    "    \n",
    "    \n",
    "    1.One-Hot Encoding\n",
    "    \n",
    "    eg - there are 5 sentences , calculate the corpus and vocabulary.\n",
    "    vocabulary = people watch campusx write comment.\n",
    "    Lets calculate the vector for document 1.\n",
    "    d1 = people watch campusx \n",
    "    d1 = [[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0]]\n",
    "    \n",
    "    Disadvantages\n",
    "    -Sparsity - if there are n no of words in the data , then this technique is not feasible.\n",
    "    it creates a sparse array and it creates overfitting as well .\n",
    "    - No fixed size\n",
    "    - Out of vocabulary\n",
    "    - No capture of semantic meaning - when we plot the vectors on graphs the semantic meaning is not captured.\n",
    "    \n",
    "    \n",
    "    2. Bag of Words\n",
    "    \n",
    "    - Create vocabulary - unique words of data and bag of words will check the frequency of vocabulary in every sentences.\n",
    "    \n",
    "     vocabulary - people,watch,campusx,write,comment\n",
    "    d1 = campusx watch campusx\n",
    "    \n",
    "    vector = [0,1,2,0,0]\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cv=CountVectorizer()\n",
    "    bow=cv.fit_transform(df['text'])\n",
    "    print(cv.vocabulary_)\n",
    "    print(bow[0].toarray())\n",
    "    \n",
    "    Advantages\n",
    "    - No fixed size  has been resolved\n",
    "    - Semantic meaning has been handled to some extent.\n",
    "    \n",
    "    Disadvantages\n",
    "    1. Sparsity\n",
    "    2. OOV - if some new words are introduced then it will be ignored.\n",
    "    3. Ordering - meaning of sentence is not captured.\n",
    "    4. If there are 2 statement\n",
    "    eg - this is a very good movie , this is not a good movie \n",
    "    and if 'not' is not present in vocabulary then if we draw vector for both statement then the meaning will be same which is actually not.\n",
    "    \n",
    "    \n",
    "    3.N-grams\n",
    "    a) Unigram  - Vocabulary will be created on single words\n",
    "    b) Bi-gram - Vocabulary will be created on 2 words\n",
    "    c) Tri-gram- Vocabulary will be created on 3 words\n",
    "    d) n-gram - Vocabulary will be created on n words\n",
    "       \n",
    "    cv= CountVectorizer(ngram_range=(1,2))\n",
    "    bow= cv.fit_transofmr(df['text'])\n",
    "    - it will create vocab list of 1 and 2 words.\n",
    "    \n",
    "    cv= CountVectorizer(ngram_range=(2,2))\n",
    "     - it will create vocab list of only 2 words\n",
    " \n",
    "    cv= CountVectorizer(ngram_range=(1,3))\n",
    "    - it will create vocab list of 1,2,3 words.\n",
    "    \n",
    "    \n",
    "    Advantages\n",
    "    - Able to capture the semantic of the sentence\n",
    "    - Easy to implement.\n",
    "    \n",
    "    Disadvantages\n",
    "    - no of vocab in unigram < no of vocab in bi-gram.\n",
    "    due to which it slows the algorithm.\n",
    "    \n",
    "    \n",
    "    4.TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "    \n",
    "    - It gives diff weightage to different words.\n",
    "        \n",
    "    TF = no of occurence of term t / total no of terms \n",
    "    - Tells particular words occurence in single document d1.\n",
    "    \n",
    "    IDF = LOG(total no of docs / no of docs)\n",
    "    - Tells particular words occurrence in entire document.\n",
    "    \n",
    "    So,for every words in document , the TF-IDF will be = Tf*Idf.\n",
    "    \n",
    "    Code Execution - \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf= TfidVectorizer()\n",
    "    tfidf.fit_transform(df['text']).toarray()\n",
    "\n",
    "\n",
    "    Disadvantages\n",
    "    1. Semantic meaning is not achieved\n",
    "    2. Sparsity is available.\n",
    "    3. Out Of Vocabulary is available.\n",
    "\n",
    "\n",
    "    \n",
    "    5.Word2Vec\n",
    "    \n",
    "    Word Embeddings - Words is converted into vector , such that the words that are closer in vector space are expected to be similar in meaning.\n",
    "    \n",
    "    Types of Word Embeddings\n",
    "    1. Frequency based\n",
    "    - Bag of words(BOW) , TF-IDF , Glove(global vector)\n",
    "    \n",
    "    2.Prediction Based\n",
    "    - Word2Vec\n",
    "    \n",
    "\n",
    "    - Word2Vec\n",
    "    - It was introduced by Google Engineer in 2013\n",
    "    Advantages\n",
    "    - Semantic meaning is captured.\n",
    "    - Dense vector is created - non- zero values will not be available.\n",
    "      - overfitting is avoided .\n",
    "    - Low dimension vector is created.\n",
    "\n",
    "    \n",
    "    -Word2vec creates features based on Vocabulary\n",
    "    \n",
    "    vocabulary - king,queen , man , woman , monkey.\n",
    "    features created will be - gender,wealth,weight,power,speak.\n",
    "    \n",
    "    so, vector will be created based on both combination.\n",
    "    \n",
    "\n",
    "    - Assumption of word2vec is that 2 words sharing similar contexts also share similar meaning and consequently similar vector representation.\n",
    "\n",
    "    - Types of word2vec\n",
    "    - CBOW - continuous bag of words.\n",
    "    - Skip-gram \n",
    "    \n",
    "    \n",
    "    CBOW - Continuous Bag of Words\n",
    "   \n",
    "    eg- watch campusx for data science\n",
    "    - This text needs to be converted to vector using One-hot encoding.\n",
    "    \n",
    "       x               y\n",
    "    watch,for     -  campusx \n",
    "    campusx,data  -  for    \n",
    "    for,science   -  data \n",
    "    \n",
    "    This data need to be trained with deep learning and check the o/p and try to minimize the cost function.\n",
    "    \n",
    "    \n",
    "    SkipGram\n",
    "    \n",
    "    x           y\n",
    "    campusx  - watch,for\n",
    "    for      - campusx , data\n",
    "    data     - for , science.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd35cb2",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "    - POS tagging is task of labelling each word in sentence with appropriate parts of speech.\n",
    "    \n",
    "    Application of POS Tagging\n",
    "    1.Named Entity Recognition\n",
    "    - It would extract the entity from the sentence. \n",
    "    eg - I would like to go to PVR Cinema , Rahul will go to bangalore tommorrow.\n",
    "    entities are - pvr cinema , rahul , bangalore , tommorrow.\n",
    "    \n",
    "    2.Question Answering System\n",
    "    \n",
    "    3.Word sense disambiguation\n",
    "    - A single word can have diff meaning in diff sentences.At that time POS tagging is used.\n",
    "        \n",
    "    4.Chatbots\n",
    "    \n",
    "    \n",
    "    Implementation\n",
    "    import spacy\n",
    "    lp=spacy.load('en_core_web_sm')\n",
    "    doc=nlp(\"I will google about facebook\")\n",
    "    \n",
    "    for word in doc:\n",
    "        print(word.text,\"--->\",word.pos_,word.tag_,spacy.explain(word.tag_))\n",
    "        \n",
    "    -it will tell the parts of speech for every word .\n",
    "    \n",
    "    \n",
    "    \n",
    "### How POS Tagging Works\n",
    "    \n",
    "    - Hidden Markov Models\n",
    "    \n",
    "    - Calculate Emisson Probability and Transition Probability.\n",
    "    \n",
    "    \n",
    "    Nitish loves campusx - Start,noun,verb,noun,end\n",
    "    can nistish google campusx - start , model , noun, verb , noun ,end\n",
    "    will ankita google campusx - start , model , noun , verb , noun,end\n",
    "    ankita loves will - start , noun , verb , noun ,end\n",
    "    will loves google - start , noun , verb , noun ,end.\n",
    "    \n",
    "    \n",
    "    create emisson probability\n",
    "    \n",
    "                 noun  model  verb\n",
    "     nitish      2/10  0       0\n",
    "     loves        0      0      3/5\n",
    "     campusx     3/10    0     0\n",
    "     google      1/10    0     2/5  \n",
    "     will         2/10   1/2   0\n",
    "     ankita      2/10    0        0\n",
    "     can           0      1/2     0\n",
    "     \n",
    "     \n",
    "    create transition probability\n",
    "    \n",
    "                n       m      v      e\n",
    "    start     3/5      2/5    0       0\n",
    "    noun      0        0     5/10      5/10\n",
    "    model      2/2     0      0       0\n",
    "    verb     5/10      0      0       0 \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da6c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcfb236f",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "\n",
    "## What is a Named Entity?\n",
    "\n",
    "    Any word which reprsents a person, organization, location etc. is a Named Entity. Named entity recognition is a subtask of Information Extraction and is the process of identifying words which are named entities in a given text. It is also called entity identification or entity chunking\n",
    "\n",
    "### Example\n",
    "    \"Apple acquired Zoom in China on Wednesday 6th May 2020\"\n",
    "\n",
    "    Here named entities are Apple, Zoom, China and Wednesday 6th May 2020\"\n",
    "    Named entity recognition is the task of identifying these words from the text\n",
    "\n",
    "\n",
    "## Why it is important?\n",
    "\n",
    "    In order to understand the meaning from a given text (for ex a tweet or document), it is important to identify who did what to whom. Named entity recognition is the first task of identifying the words which may represnt the who, what and whom in the text. It helps in identifying the major entities the text is talking about\n",
    "\n",
    "    Any NLP task which involves automatically understanding text and acts based on it, needs Named Entity Recognition in its pipeline\n",
    "\n",
    "    Caveat\n",
    "    No algorithm can 100% identify all the named entities correctly\n",
    "\n",
    "## Three approaches\n",
    "\n",
    "    Basic NLTK algorithm\n",
    "        with word segmentation\n",
    "        with sentence segmentation\n",
    "    Stanford NLP NER\n",
    "    Using Spacy\n",
    "\n",
    "\n",
    "\n",
    "### What you can build with this?\n",
    "\n",
    "    A bot that can analyze financial news and extract information about entities that are mentioned in a given article along with location, dates and numeric information. This information can be further utilized in building algorithmic trading bots\n",
    "\n",
    "    Analyze research papers produced everyday on COVID19 and find out any significant developments\n",
    "\n",
    "\n",
    "\n",
    "### Named Entity is a real world or abstract object such as persons, location , organization , products etc that can be denoted with proper name.\n",
    "\n",
    "\n",
    "### Named Entity Recognition \n",
    "    - It is subset of information extraction.\n",
    "    - Find and classify named entity mentioned in unstructured text and classify into categories such as person names ,organizations , locations, medical codes , time expressions , quantities , values , percentages , etc.\n",
    "    \n",
    "    \n",
    "    \n",
    "### Spacy\n",
    "\n",
    "    - Spacy is free , open source library for advanced NLP  in python\n",
    "    - Spacy is designed specifically for production use and helps u build applications that process and understand large volume of texts. \n",
    "    \n",
    "    - It can be used to build information extraction or natural language understanding systems or to preprocess text for deep learning .\n",
    "    \n",
    "    \n",
    "    \n",
    "## NLP Library\tDescription\n",
    "\n",
    "\n",
    "    NLTK -\tThis is one of the most usable and mother of all NLP libraries.\n",
    "\n",
    "    spaCy -\tThis is a completely optimized and highly accurate library widely used in deep learning\n",
    "\n",
    "    Stanford CoreNLP Python\tFor client-server-based architecture, this is a good library in NLTK. \n",
    "    This is written in JAVA, but it provides modularity to use it in Python.\n",
    "\n",
    "    TextBlob -\tThis is an NLP library which works in Pyhton2 and python3. \n",
    "    This is used for processing textual data and provide mainly all type of operation in the form of API.\n",
    "\n",
    "    Gensim\t- Genism is a robust open source NLP library support in Python. This library is highly efficient and scalable.\n",
    "\n",
    "    Pattern - \tIt is a light-weighted NLP module. \n",
    "    This is generally used in Web-mining, crawling or such type of spidering task. \n",
    "\n",
    "    Polyglot- For massive multilingual applications, Polyglot is best suitable NLP library. \n",
    "    Feature extraction in the way on Identity and Entity.\n",
    "\n",
    "    PyNLPl - \tPyNLPI also was known as ‘Pineapple’ and supports Python. \n",
    "    It provides a parser for many data formats like FoLiA/Giza/Moses/ARPA/Timbl/CQL.\n",
    "\n",
    "    \n",
    "    Vocabulary - \tThis library is best to get Semantic type information from the given text.    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2d38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f7e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b79dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad0f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573cace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc38252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a0c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcc33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d0d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64357e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99611eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fbe70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ef2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3c50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ba887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2927b10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602627b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e0535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b5b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc26a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bbfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a553a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d12978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb3837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0240ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158080c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2aa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544c025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9a90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf7f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbe8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98fa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070714c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe2245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814e255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20749f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73be303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127cf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2950a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ad84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94007c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29430e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf7c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fb5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ec9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0897618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de9fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca0421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
