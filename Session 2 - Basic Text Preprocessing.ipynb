{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d747c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc4dfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:\\Sandesh\\Data Science\\Class Assignment\\Codes\\All DataSets\\Kaggle Dataset\\IMDB Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b35e7",
   "metadata": {},
   "source": [
    "### 1.  Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a92319e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the entire text into lower case.\n",
    "df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b804d2",
   "metadata": {},
   "source": [
    "### 2. Remove the HTML  tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae913d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. The filming tec...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whenever we scrap data from online sites , there are html tags available.\n",
    "# Regex101.com site is available which can give o/p for regular expression\n",
    "# <br> tag has been removed \n",
    "\n",
    "\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "df['review'].apply(remove_html_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeabc2d",
   "metadata": {},
   "source": [
    "### 3.  Remove the URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9817eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the url we need to have the understanding of regular expression.\n",
    "# here we have to write the regular expression based on the data that we have.\n",
    " \n",
    "def remove_url_tags(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+') \n",
    "    return pattern.sub(r'',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8040200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n",
      "Check out my notebook \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' to search check '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Check out my notebook https://www.kaggle.com/campus/notebook\"\n",
    "text2 = \"Check out my notebook http://www.kaggle.com/campus/notebook\"\n",
    "text3 = \"Check out my notebook www.kaggle.com/campus/notebook\"\n",
    "text4 = \"https://www.kaggle.com/campus/notebookfvcs to search check www.google.com\"\n",
    "\n",
    "print(remove_url_tags(text1))  \n",
    "print(remove_url_tags(text2)) \n",
    "print(remove_url_tags(text3))  \n",
    "remove_url_tags(text4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a33ff",
   "metadata": {},
   "source": [
    "### 4.   Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b082fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation will not help in anyway to understand the sentence meaning\n",
    "# While tokenizing it may happen that words&punctuation will act as single words.\n",
    "    \n",
    "# eg: Hello! and Hello - machine will think both of the words are not equal .\n",
    "    \n",
    "import string\n",
    "import time\n",
    "\n",
    "string.punctuation   # List of punctuation available     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063f7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation\n",
    "\n",
    "# Check if particular punctuation is available in text , \n",
    "# if available then replace it.\n",
    "\n",
    "def remove_punc_1(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b412716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This technique will take more time if we are working with more datasets.\n",
    "   \n",
    "def remove_punc_2(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ca076",
   "metadata": {},
   "source": [
    "### 5.  Chat word Treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "497f8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your data contains some shortcuts , at that time we need to go with\n",
    "# we need to have complete word in form of dictionary with word and full form \n",
    "# so that it can extract the full word.\n",
    "# eg  : {FMO:'Fear of Missing Out'}\n",
    "# chat_words is that dictionary \n",
    "\n",
    "new_text=[]\n",
    "def chat_conversation(text):\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d561d3b",
   "metadata": {},
   "source": [
    "###   6.    Spelling Correction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8262d42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions durings several generation'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Textblob is the function which is available which is used to \n",
    "# correct the sentences.\n",
    "    \n",
    "from textblob import TextBlob\n",
    "incorrect_text='certiana conditionsas durinngs severa generatin'\n",
    "\n",
    "txtBlb=TextBlob(incorrect_text)\n",
    "txtBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaa9ba",
   "metadata": {},
   "source": [
    "### 7.    Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca9852de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a , the, of , are , my - these words does not have any meaning in sentence formation.\n",
    "# In POS tagging we dont do the Stop words removal \n",
    "\n",
    "# To check the stopwords in english dictionary, we can check for other languages as well .\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "            \n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eab38cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probaly  all-time favourite movie,  story  selflessness ,an sacrifice  dedication  noble cause'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probaly my all-time favourite movie, a story of selflessness ,an sacrifice and dedication to noble cause ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f05ea2",
   "metadata": {},
   "source": [
    "###  8.    Handling emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2876ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#There is function available to remove the emojis\n",
    "    \n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"{ \" )  \n",
    "    # some utf-code has to be passed check the videos\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bb1a6",
   "metadata": {},
   "source": [
    "###  9.      Tokenization - Breaking the sentence into smaller parts \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b00d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 types of tokenization are available : Words and sentence tokenization\n",
    "# In word tokenization , there are some sentence where tokenization fails \n",
    "# thus we have to deal with it carefully.\n",
    "\n",
    "# Techniques for tokenization\n",
    "\n",
    "# 1. Simple techniques  : Split function but it will not work evey time \n",
    "# accurately depends on the text that we have .\n",
    "\n",
    "# 2.Regular expression : More Accurate compare to split function\n",
    "\n",
    "# 3.NLTK : Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dac6e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "text1 = 'I am going to visit delhi'\n",
    "word_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd06c9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorus is simply dummy text of printing and typesetting industry?',\n",
       " 'Lorus has been the industry standard dummy text ever since 1500.',\n",
       " 'Ahen an unknown printer took gallery of type and scambbled it to make a type specimen book.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sent = \"\"\"Lorus is simply dummy text of printing and typesetting industry?\n",
    "Lorus has been the industry standard dummy text ever since 1500.\n",
    "Ahen an unknown printer took gallery of type and scambbled it to make a type specimen book.\"\"\"\n",
    "sent_tokenize(text_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97d1bbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5 = 'I have a ph.D in A.I'\n",
    "sent6 = \"We're here to help mail us at nks@gmail.com\"\n",
    "sent7 = 'A skinnny ride cost $10.50'\n",
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57b46cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here email id is not considered as single word.\n",
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69a057f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'skinnny', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b2f2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# If data is complex then go with spacy otherwise can use some other techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c8f6445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here it is considering email id as single word \n",
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "    \n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebd6170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "# ph.d is considered into 3 words , which is incorrect.\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a67802",
   "metadata": {},
   "source": [
    "### 10.  Stemming  - Mapping group of words to their root word but does not have specific meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5cf4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "    \n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "383bb0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7c8af",
   "metadata": {},
   "source": [
    "### 11.   Lemmatization -   Mapping group of words to their common words and have specific meaning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61d8fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stemming - Fast\n",
    "# Lemmatization - Slow \n",
    "\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "    \n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35920531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: studi\n",
      "Lemmatization: studying\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming:\" ,ps.stem('studying'))\n",
    "print(\"Lemmatization:\" ,lemmatizer.lemmatize('studying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61f20822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: dens\n",
      "Lemmatization: densely\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming:\" ,ps.stem('densely'))\n",
    "print(\"Lemmatization:\" ,lemmatizer.lemmatize('densely'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5e67a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: commonli\n",
      "Lemmatization: commonly\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming:\" ,ps.stem('commonly'))\n",
    "print(\"Lemmatization:\" ,lemmatizer.lemmatize('commonly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e62c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a78b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2f25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33f6cb84",
   "metadata": {},
   "source": [
    "### THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
