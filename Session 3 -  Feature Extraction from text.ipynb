{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f36f93",
   "metadata": {},
   "source": [
    "### Feature Extraction  - Convert text into vectors \n",
    "\n",
    "- Corpus          -  Collection of all words as well as duplicate words in the data. \n",
    "- Vocabulary      -  Unique words in the entire data. \n",
    "- Document        -  Sentiment analysis of twitter data , every individual review is a document. \n",
    "- Word           -   word in sentences.\n",
    "\n",
    "\n",
    "### To Convert Text into Vectors \n",
    "    \n",
    "1. One-Hot Encoding\n",
    "2. Bag of words\n",
    "3. Ngram\n",
    "4. TF-IDF\n",
    "5. Custom Features\n",
    "6. Word2Vec - Embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99c364a",
   "metadata": {},
   "source": [
    "### 1.  One-Hot Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c737f0",
   "metadata": {},
   "source": [
    "- Eg - there are 5 sentences , calculate the corpus and vocabulary.\n",
    "    \n",
    "- Vocabulary = people watch campusx write comment.\n",
    "    \n",
    "- Lets calculate the vector for document 1.\n",
    "    \n",
    "    \n",
    "    \n",
    "    d1 = people watch campusx \n",
    "    d1 = [[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0]]\n",
    "    \n",
    "    Disadvantages\n",
    "    -Sparsity - if there are n no of words in the data , then this technique is not feasible.\n",
    "    it creates a sparse array and it creates overfitting as well .\n",
    "    - No fixed size\n",
    "    - Out of vocabulary\n",
    "    - No capture of semantic meaning - when we plot the vectors on graphs the semantic meaning is not captured.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5da28",
   "metadata": {},
   "source": [
    "### 2. Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08717973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary - unique words of data and bag of words will check the frequency of vocabulary in every sentences.\n",
    "    \n",
    "# Advantages\n",
    "#1. No fixed size  has been resolved\n",
    "#2. Semantic meaning has been handled to some extent.\n",
    "    \n",
    "# Disadvantages\n",
    "# 1. Sparsity\n",
    "# 2. OOV - if some new words are introduced then it will be ignored.\n",
    "# 3. Ordering - meaning of sentence is not captured.\n",
    "# 4. If there are 2 statement\n",
    "#eg - this is a very good movie , this is not a good movie \n",
    "#    and if 'not' is not present in vocabulary then if we draw vector for both statement then the meaning will be same which is actually not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afd4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53532298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campusx watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campusx write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch campusx       1\n",
       "1  campusx watch campusx       1\n",
       "2   people write comment       0\n",
       "3  campusx write comment       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'text':['people watch campusx','campusx watch campusx',\n",
    "                       'people write comment','campusx write comment'],\n",
    "               'output':[1,1,0,0]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041681d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create vocabulary for the dataframe\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "bow = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d02b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2655ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Checking vocabulary for first sentence.\n",
    "print(bow[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfb8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c6a7e",
   "metadata": {},
   "source": [
    "### 3.  N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0e5cb",
   "metadata": {},
   "source": [
    "1. Unigram  - Vocabulary will be created on single words\n",
    "2. Bi-gram - Vocabulary will be created on 2 words\n",
    "3. Tri-gram- Vocabulary will be created on 3 words\n",
    "4. n-gram - Vocabulary will be created on n words\n",
    "       \n",
    "    cv= CountVectorizer(ngram_range=(1,2))\n",
    "    bow= cv.fit_transofmr(df['text'])\n",
    "    - it will create vocab list of 1 and 2 words.\n",
    "    \n",
    "    cv= CountVectorizer(ngram_range=(2,2))\n",
    "     - it will create vocab list of only 2 words\n",
    " \n",
    " \n",
    "     cv= CountVectorizer(ngram_range=(1,3))\n",
    "    - it will create vocab list of 1,2,3 words.\n",
    " \n",
    " \n",
    "     Advantages\n",
    "    - Able to capture the semantic of the sentence\n",
    "    - Easy to implement.\n",
    " \n",
    " \n",
    "     Disadvantages\n",
    "    - no of vocab in unigram < no of vocab in bi-gram.\n",
    "    due to which it slows the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025fafd",
   "metadata": {},
   "source": [
    "### 4.    TF - IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0133abcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}\n"
     ]
    }
   ],
   "source": [
    "# It gives diff weightage to different words.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv=TfidfVectorizer()\n",
    "bow = cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e1c65",
   "metadata": {},
   "source": [
    "### 5.   Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc26415",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word Embeddings - Words is converted into vector , such that the words that are closer in vector space are expected to be similar in meaning.\n",
    "    \n",
    "    Types of Word Embeddings\n",
    "    1. Frequency based\n",
    "    - Bag of words(BOW) , TF-IDF , Glove(global vector)\n",
    "    \n",
    "    2.Prediction Based\n",
    "    - Word2Vec\n",
    "    \n",
    "\n",
    "    - Word2Vec\n",
    "    - It was introduced by Google Engineer in 2013\n",
    "    Advantages\n",
    "    - Semantic meaning is captured.\n",
    "    - Dense vector is created - non- zero values will not be available.\n",
    "      - overfitting is avoided .\n",
    "    - Low dimension vector is created.\n",
    "\n",
    "    \n",
    "    -Word2vec creates features based on Vocabulary\n",
    "    \n",
    "    vocabulary - king,queen , man , woman , monkey.\n",
    "    features created will be - gender,wealth,weight,power,speak.\n",
    "    \n",
    "    so, vector will be created based on both combination.\n",
    "    \n",
    "\n",
    "    - Assumption of word2vec is that 2 words sharing similar contexts also share similar meaning and consequently similar vector representation.\n",
    "\n",
    "    - Types of word2vec\n",
    "    - CBOW - continuous bag of words.\n",
    "    - Skip-gram \n",
    "    \n",
    "    \n",
    "    CBOW - Continuous Bag of Words\n",
    "   \n",
    "    eg- watch campusx for data science\n",
    "    - This text needs to be converted to vector using One-hot encoding.\n",
    "    \n",
    "       x               y\n",
    "    watch,for     -  campusx \n",
    "    campusx,data  -  for    \n",
    "    for,science   -  data \n",
    "    \n",
    "    This data need to be trained with deep learning and check the o/p and try to minimize the cost function.\n",
    "    \n",
    "    \n",
    "    SkipGram\n",
    "    \n",
    "    x           y\n",
    "    campusx  - watch,for\n",
    "    for      - campusx , data\n",
    "    data     - for , science.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5340a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61d493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b44ec900",
   "metadata": {},
   "source": [
    "### THE END "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
